|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
MON1022|IV record 2a 2b|MoE survey; CLRS 2nd|统机: EM|Code:MAUC|English:ashfile park, test personality|EM program: 3 coins|
output|in terms of = when it comes to = as far as __ is concerned|the formulation, $P$ can denote different networks but need further explanation|3 coins, mixture gaussian; observed/hidden variable|D:\UserData\IT\python\DiyKaggleBook\chapter2\my_MAUC.py|chase/breas  |D:\zCode\Python\ML_test\SM_LH_3coins.py one epoch converge ; http://docs.anaconda.com/anaconda/user-guide/getting-started/|
Next| use sentence patterns |test with kaggle data(MAUC)|proof|the paper MAUC is micro or macro|tomorrow morning|how to derivate
|-------|-------|-------|-------|-------|-------|-------|
TUR1023|English,EM derivation| | | |English discuss|only amazon
output|flatmate 1023 human body,definition 9.1 Q function in SM,16personality onedrive/Doc/dailylearning| | | |kate flatmate | AUD: mic amazon 555 mac mean
Next| | | |

|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
WEB1024|jenson plot; psy; rich|GP, Bayesian Network|PRML: Probablity,likelihood||USML:PAC|
output|baidu; kindle:reason;choice;friends;master/learn new formula|LTnote |LTnote ||draft-LTnote |
Next|;papa(wise,courage,reliable),stamina|Kernel, model|go on||D^m Corollary3.2 large or small sample |

note of WEB1024:
1. reading GP Bayesian Network: I found I need learn the Probability, likelihood first.
1. typing and derivaing the Bernouli model take a class in the morning
1. in fact: writing the USML takes afternoon a class, typing takes the evening 2 classes


|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
THU1025|PRML: probability|intensive English|PRML:exponential family 2.3Gaussian distribution
output|LTnote: beta, multinomial, 1-of-K writing|pronunciation(exersice4), prasentation(begin) |LT:distance;Jacobian; normalization
Next|normalization, pratice lagrange, dirichlet: nornalization|flatmate discussion + monday meeting | nonparametric method:kernel

1. last night xo, so this morning not so high-efficient.  1.1--->11:30 revist -->12:00
2. make and eat the lunch half hour movie 1:30. 2.1->4:30 
3. monday meeting, the review + submision + MoE, so today, Gaussian Process + assigment(wrting booklelt)


|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
FRI1026|PRML: revisit; non-parametric; histogram|same|kernel->gaussian kernel|learn/buy|discussion|GP 6.4|
output|LT: writing|distraction :(|good, why kivi slow
Next|GP|don't meaningless news | skip KNN, go to GP!!

1. wake up early, but in 1.2 I was distracted from study
2. relax well in noon, even with other thing, afternoon very good!!
3. 


|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
SAT |PRML: kernel,linear,construct new|gaussian kernel|probabiliy fisher|~|~|UDML:16 kernel trick
output|LT: |LT: from construct-->valid|LT: one kind of kernel|~|~|LT: process/eg. of kernel
Next|back to regression|RBF|how link|~|~|express prior knowledge

when editing in a large .md file, I find it is very slow. So i turn to a temperary short file, and then copy to the LT files


|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
SUN 1028 |PRML: dual representation/regression|rich 5 self-discipline|w in Guassian|Church|$\sigma$-filed; GP.pdf|PRML, paper|
output|LT: revisit, for GP|also apply in other |LT|~|
Next|GP, reorganize, matrix square and derivative|to flatmate.md|hang on|shreey|reorganize|conclusion, link 2.3.1 2.3.3|

1. at 9:23, I found: 6.4.2 more clear, try to find why at 1st time, 6.4.2 is so difficult?
   - ans: because I don't what the index point to. I mix up the different observations of one variable and different variable in one process
   - and more originating reason, I didn't write it on the paper
2. C4.5 in LHML contents.


|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
MON 1029|PRML:condition/joint Gaussian|algebra|~|bundy|what delay..|ppt|
output|Long term|long term|~|bundy|~|focuse|
Next||



|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
THU 1029|PPT|PPT|PPT|meeting|annotate the ppt|~
output|Long term|long term|~|bundy|~|focuse|
Next||||

1. the styles of supervisors are different, you should accommodate yourself to the different styles and make good use of them. 

|time|1.1|1.2|2.1|2.2|3.1|3.2|
|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
THU 1029|how do others write, and ask about the plan|~
output|Long term|long term|~|bundy|~|focus|

1. is this article good for you? https://academic.oup.com/economicpolicy/article-abstract/9/18/101/2392430?redirectedFrom=fulltext
2. journal doesn't have deadline, except special issue

3. you must get the opportunity, or everything will fail!! Be nice, tolerant to your girl, there are no equal!!
    1. want to welcome, but still refuse. You should know this natural! using your courage, wise and tolerant!!


before the moment you moved in, the water facility was leak and block, and after the moment you moved out the electriciy broken. These let think of you are very special.

not interfere damage the property or bring trouble to others anymore, you can only bring some surprise in my house or flat.

1. Timetable:
IJCAI deadline Feb-25, so time table:
Jan. finish writing; 
Dec. complete all the experiment; 
Nov. literature reviews ---- it is next month, so it also help the writing of TNNLS, as I can learn how some good papers were written.

2. difference:
Another issue is what differences among IJCAI(more sponsor from China),AAAI,ICDM(should be similar to KDD),TNNLS(it is system level and focus on Neural network), how can I study the differences and make use of them?
From previous learning: some TNNLS or other transaction papers about emerging research topics that are very theoretical the experiments are more systematic(like prof. Yao's style), and NIPS, AAAI, ICDM papers solve certern problems in specific topic, eg. imabalanced image data classification and boundary detection, Improved Gans and some variants of model in specific applications (like your previous instructions).
3. topic and strength:
For the from the final PhD thesis, breaking down to the research topic and one-by-one projects. We should begin with the strength of the panel, supervisors and myself. 
As in Chen's opinion, I have 2 very great supervisors, and myself is also not bad. But for the previous long period, the topic and projects were irrelevant to the strength of supervisors even not to mine. 
The joint-PhD is like a reform for me, the reform should begin with the strength (Learn China's reform v.s. Former Soviet's reform), get some results, and only then be able to turn to key-weakness or other high-valuable and challenging projects.  I found the previous Phd student prof. Yao mentioned is researching online imbalanced learning where both class imbalance and concept drift coexist. 
Therefore, as a long-term topic and one-by-one projects,  I am interested in the combined problem of both imbalanced and transfer learning(it is also relevant to the concept drift). 
If you agree, can you recommend a list of papers in this field? it will be better if they are your student's papers, as it lets the discussion much easier.  


|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
Wedn 1031|discuss future|reading and annotated|send|english story with picture|ready for meeting about TRM
|output|begin with PoP then Ivor determine which is better|~||
|next|PoP search quickly|write and summary|xx puts money in your pocket|refine|language|



1. patience and stamina help me a lot. Don't assume they are bad guy in the first engage because of your over-reaction. Don't send the situmiling information, but double-check what you really need and change it, more polite, or ask question in self-examination manner. Even it is the bad thing you feel, try to ask and comfirm first. if you self-exam well, your question will also let him/her self-exam.
2. face your weakness, the training and tackling it is the easy part once you begin. Although the identify and admit it are the hardest part, if you are strong and wise, it is just a moment.  --- accept and embrace and tackle
3. you learn a lot in telling the stories using the photo. but you still need to learn to refine it.
    1. spurt/spray my suitcase onto Conveyor belt and hit the boundary.


|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
Thur 1101|listening, accepted paper|get an ideal, summary the good paper in 2 pages|
|output|script, list
|next|

1. deal with the final decision: they said the idea is well-known, me it is like implement ...
[googl][https://www.google.com.au/search?q=The+idea+of+synthetic+images+for+imbalanced+learning&rlz=1C1CHBF_en-GBAU811AU811&oq=The+idea+of+synthetic+images+for+imbalanced+learning&aqs=chrome..69i57.353j0j4&sourceid=chrome&ie=UTF-8]
1. how han bo's TNNLS published:
   Progressive Stochastic Learning for Noisy Labels
    B Han, IW Tsang, L Chen, C Yu, SF Fung
    IEEE Transactions on Neural Networks and Learning Systems
1. Beyond Majority Voting: A Coarse-to-Fine Label Filtration for Heavily Noisy Labels
B Han, IW Tsang, L Chen, JT Zhou
Submitted to IEEE Transactions on Neural Networks and Learning Systems

1. two nips:  same filed and related, how to conduct them at the same time and get the good results 
  Masking: A New Perspective of Noisy Supervision
    B Han, J Yao, G Niu, M Zhou, IW Tsang, Z Ya, M Sugiyama
        Advances in Neural Information Processing Systems
1. Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels
    B Han, Q Yao, X Yu, G Niu, M Xu, W Hu, IW Tsang, M Sugiyama
        Advances in Neural Information Processing Systems


|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
Thur 1101|listening, accepted paper|get an ideal, summary the good paper in 2 pages|
|output|script, list
|next|

1. think you are not very quick as plan. eg. plan: 7-8am: revist one TNN and read another TNN + listen, but in fact, only revisit ...   --> quickly, or plan less
2. orinal plan:  7-8 revisit, get the writing;  8-10 assigment 3; 10-12 report
3. something you need to consider logical. eg. VPN -> ask classmate. actually if you consider logically, it is no need.  google is a better option.c
ready for this paper
https://ieeexplore.ieee.org/abstract/document/6549126



# convergence and robustness analysis
1. Bond: convex optimization :
    1. search convergence in the index I get the context:
        - if I list from basic to complex: linear 467/quadratic 489 539/convex hull536/newton method 529 536/barrier cone 536 577.  Probably I will try to understand the 1st 3 analysis and incoroperate with the referrence paper of your papers that I am reading, 
        - what is infeasible newton methods ...

http://web.stanford.edu/~boyd/books.html
    convex as well and linear analysis

http://www.cs.princeton.edu/~ehazan/tutorial/tutorial.htm
Online Convex Optimization A Game-Theoretic Approach to Learning    --- related to my model

noisy, shoatage, imbalance, transfer: what is the relation and novelty.  old problem new method is it novelty?

|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
Fri 1102|discuss convergance with boss|~|assigment|convex|convex|CV
|output|the list|deviate to news..|submitted|LT:
|next|book+ a link|xx|yes|yes|next time, discussion ! make appointment! and draft


https://www.seek.com.au/job/37385464?type=standard
lea

1. learning from xin yao to edit your CV: 
so many rewards, leading role.  supervior role.



|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
Sat 1103|convex|~|!
|output|good|
|next|in the convex book+LT|~|~|you must your self-disipline next time!!


|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
Sun 1104|travel|~|~|~|rich|~|backtracking line searsh
|output|flatmate|~|~|~|~|step 7. your hero; step 8 teaching and giving|backgradient

1. good atmosphere help to learn and communicate
2. you should show your self-displine


|time   |1.1  8:30-9:00; 9:00-9:15 |1.2 10:10-11:20 steep descent   |1.3 11:35~13:00    |2.1 02:40-04:35 2hour ...|2.2 04:35-5:20 logic part, listen/read/reply|3.2    |
|-------|-------|-------|-------|-------|-------|-------|
|Mon 1104|a problem on R^2; independent note in LT|steep|back to paper|
|output|note in the pdf, in LT|LT|Log_paper; | ananlysis
|next|CV just ready|~|screen loss| the two important inquality|

1. 9:15, so far the problem is only the level set. There are no training set, so it is not the classification or regression problem.   least-square regression is the also quadratic problem? so how is the classificatoin?
    1. ans: yes, eg. problem with 100 variables can be also quadratic as the high power is 2. eg. $x_1^2,x_1,x_{99}, x_{99}^2$
2. 9:15~10:10 eating, because you do the reading at the same time? next time XX.  close the wechat, if you do not communicate with surpervisor.  making the lunch  15 mius
3. 10:10~11:20 steep descent
4. morning, do as much as gradient as you can, but read the paper and organize the logic ASAP.  paper, and then steep gradient and newton method.  Steep first because it is related to gradient method. then paper, then newton
5. when read the paper, I spend so many time to understand the notation, and the define. -- your note should cover this, then in the future, revisiting the paper is quite easy!!
6. lunch + play is too long 1:40, + 10min rejuvenating my eyes!! today, no small sleep.
7. ready the paper, is very very slow !! but I get the logic to write the analysis !!
8. 04:35 - 05:20 40 min, only 1/4 the meeting record.
9. eating and discussion. now is 9:00.



|time   |1.1 7:00-8:30   |1.2 10:40  ~2.1 3:15  |2.2 03:30    |3.1|3.2    |
|-------|-------|-------|-------|-------|-------|-------|
Tue 1106|listening report|paper(HanBo) report|PRML: bayes network|report, modify comments in bo paper, change to my 2 pages paper|
|output|log_paper. clearly, as for|note + discuss with boss about the UCI data|slow, unhappy|~|
|next|write the report|desige your experiment + shuowang|

1. shuowang, all ensemble learning in the experiment.
1. the book of ensemble methods
1. don't be angry!! do your best first. don't let the world ruled by such bad guy. Specially, finished your report(main points), and 2 page pepers, send email + your questions.  
   - and then what problem, you just need to list it logically. (how often you contact; his trick(drop); when give me suggestion(last hour), what is your sincere(100 vs 1); ) 
   - Which one problem we should report bad guy? which one discuss directly. Which one together. 
   - specific problems fill-in the 2 page paper is easier and well-worthy to undertand. !!!
1. write the report spends much more time than you thought!!
    - ivor's link: http://www.cs.princeton.edu/~ehazan/tutorial/tutorial.htm   slides about convex analysis/ convergence, but not about game.



|time   |1.1    |1.2
|-------|-------|-------|-------|-------|-------|-------|
Web 1107|a script for 2 page paper|
|output|whole day: 2-page paper, logic_mail, script
|next|

1. a scrip of 2 page paper
2. ready, try to fix the
3. mode score + inception score in GAN
   1. a conditional label distribution $p(y|x)$ with low entropy -- meaningful object, that is classify correctly
   2. the marginal $p(y|x = G(z))dz$ should have high entropy -- varied images
   3. entroy: $H(p)=E[-\log p_i]=\sum_{i=1}^N-p_i\log p_i$
   4. condition entroy $H(x|y)=\sum_{i=n}p_i(y)H(X|y=Y)=H(X,Y)-H(Y)$  --H(x,y) here is joint entropy, not cross entropy. if H(x,y)=0, mean the uncontainty of (x,y) is equal to y, that means  x is completely determined by the value of X.
   5. cross entropy: $H(p,q)=-\sum_{i=1}^N p_i\log q_i = H(p)+KL(p\|q)$, if cross entropy minimized, the KL is minimize.   the log likelihood is maximized.



# Thu 1109 
1. consent matter training -- let's do it tomorrow
2. quick reply Prof. Yao today
3. High dimension-feature selection -- next topic -- see his metrics first (+ merits)  the ACM KDD, and the neural computation
    1. metrics: f measure, AUC for binary classify
    2. metrics: F1(BEP) f socre: accuracy not recall
    3. term/category tuples --> metrics: information gain;  chi-square; correlation coefficients Odds ratio;  OR-square
4. fix the qt plugin problem when using matplot.
    1. the information reinstall this application -- be aware: application is not only VSCode, conda, python, but the plugin it selt is the application
    2. search answer from stackoverflow: conda remove .. conda install --- complete!!


% suffient feature selection
@inproceedings{mladenic1999feature,
  title={Feature selection for unbalanced class distribution and naive bayes},
  author={Mladenic, Dunja and Grobelnik, Marko},
  booktitle={ICML},
  volume={99},
  pages={258--267},
  year={1999}
}


|time   |1.1    |1.2
|-------|-------|-------|-------|-------|-------|-------|
Thur 1108|ask boss about the high-dimension problem|fix the 2 page|
|output|
|next|



|time   |1.1    |1.2
|-------|-------|-------|-------|-------|-------|-------|
|Fri 1109|modify the English of AdaBoost|Pami paper|PRML:Bayesian network|CL SPL|
|output|temp.md|undirected graph + convengence|directed graph, conditonal probability|
|next|+CL SPL|RPML|C5 Bayesian Neural Network,8.1.2 Generative Model|with AdaBoost|

1. Bayesian Network here factorize the joint probability over K variable.  $p(\mathbf x)=\Pi_{k=1}^Kp(x_k|pa_k)$ -- Interpret: if only p(a) p(b), a depends on b, then p(b|a)p(a|b), as b doesn't depend on a p(b|a)=p(b)

|time   |1.1    |1.2    |
|-------|-------|-------|-------|-------|-------|-------|
|Sat 1110|8.1.2generative model in graph model|8.1.3 dicrete variable|
|output|ascestral sampling|map back to one hot notation
|next||

1. $p(w|\mathbf t)\propto p(w)p(\mathbf t|w)=p(w)\Pi_{n=1}^N p(t_n|w)$  posterior is directly proportional to the joint probability = prior * conditional probability
2. only learn the format no need for later


|time   |1.1    |1.2    |
|-------|-------|-------|-------|-------|-------|-------|
|Sat 1112|PRML 8.1.3 discrete|DL 1 Algebra 20 generative model
|output|seperate to different file
|next||

1. why the patition function is intractable? in chapter 20. come back to chapter 18. is it just $\propto$ then every thing is OK?
2. make a note of reading boss's paper and then sent to boss


[to flatmate1107](./flatmate.md#1107)
