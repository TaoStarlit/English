|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
MON1022|IV record 2a 2b|MoE survey; CLRS 2nd|统机: EM|Code:MAUC|English:ashfile park, test personality|EM program: 3 coins|
output|in terms of = when it comes to = as far as __ is concerned|the formulation, $P$ can denote different networks but need further explanation|3 coins, mixture gaussian; observed/hidden variable|D:\UserData\IT\python\DiyKaggleBook\chapter2\my_MAUC.py|chase/breas  |D:\zCode\Python\ML_test\SM_LH_3coins.py one epoch converge ; http://docs.anaconda.com/anaconda/user-guide/getting-started/|
Next| use sentence patterns |test with kaggle data(MAUC)|proof|the paper MAUC is micro or macro|tomorrow morning|how to derivate
|-------|-------|-------|-------|-------|-------|-------|
TUR1023|English,EM derivation| | | |English discuss|only amazon
output|flatmate 1023 human body,definition 9.1 Q function in SM,16personality onedrive/Doc/dailylearning| | | |kate flatmate | AUD: mic amazon 555 mac mean
Next| | | |

|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
WEB1024|jenson plot; psy; rich|GP, Bayesian Network|PRML: Probablity,likelihood||USML:PAC|
output|baidu; kindle:reason;choice;friends;master/learn new formula|LTnote |LTnote ||draft-LTnote |
Next|;papa(wise,courage,reliable),stamina|Kernel, model|go on||D^m Corollary3.2 large or small sample |

note of WEB1024:
1. reading GP Bayesian Network: I found I need learn the Probability, likelihood first.
1. typing and derivaing the Bernouli model take a class in the morning
1. in fact: writing the USML takes afternoon a class, typing takes the evening 2 classes


|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
THU1025|PRML: probability|intensive English|PRML:exponential family 2.3Gaussian distribution
output|LTnote: beta, multinomial, 1-of-K writing|pronunciation(exersice4), prasentation(begin) |LT:distance;Jacobian; normalization
Next|normalization, pratice lagrange, dirichlet: nornalization|flatmate discussion + monday meeting | nonparametric method:kernel

1. last night xo, so this morning not so high-efficient.  1.1--->11:30 revist -->12:00
2. make and eat the lunch half hour movie 1:30. 2.1->4:30 
3. monday meeting, the review + submision + MoE, so today, Gaussian Process + assigment(wrting booklelt)


|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
FRI1026|PRML: revisit; non-parametric; histogram|same|kernel->gaussian kernel|learn/buy|discussion|GP 6.4|
output|LT: writing|distraction :(|good, why kivi slow
Next|GP|don't meaningless news | skip KNN, go to GP!!

1. wake up early, but in 1.2 I was distracted from study
2. relax well in noon, even with other thing, afternoon very good!!
3. 


|time   |1.1    |1.2    |2.1    |2.2    |3.1    |3.2    |
|-------|-------|-------|-------|-------|-------|-------|
SAT |PRML: kernel,linear,construct new|gaussian kernel|probabiliy fisher|~|~|UDML:16 kernel trick
output|LT: |LT: from construct-->valid|LT: one kind of kernel|~|~|LT: process/eg. of kernel
Next|back to regression|RBF|how link|~|~|express prior knowledge

when editing in a large .md file, I find it is very slow. So i turn to a temperary short file, and then copy to the LT files